{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVNlBOjP91Ou"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment | Decision Tree"
      ],
      "metadata": {
        "id": "M82SULTA97Yx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1:  What is a Decision Tree, and how does it work in the context of classification?\n",
        "\n",
        "Answer-1. A Decision Tree is a supervised learning algorithm used for classification and regression tasks. It works by recursively partitioning the data into smaller subsets based on feature values.\n",
        "\n",
        "How it Works\n",
        "1. Root Node: The algorithm starts with a root node representing the entire dataset.\n",
        "2. Splitting: The algorithm splits the data into subsets based on feature values, creating child nodes.\n",
        "3. Recursive Partitioning: The process is repeated recursively for each child node until a stopping criterion is met.\n",
        "4. Leaf Nodes: The final nodes, called leaf nodes, represent the predicted class labels.\n",
        "\n",
        "Classification\n",
        "1. Feature Selection: The algorithm selects the most informative feature to split the data.\n",
        "2. Splitting Criteria: The algorithm uses a splitting criterion, such as Gini impurity or entropy, to determine the best split.\n",
        "3. Class Prediction: The predicted class label is determined by the majority class in the leaf node.\n"
      ],
      "metadata": {
        "id": "pwR3f2Q3-S_3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?\n",
        "\n",
        "Answer-2. Gini Impurity\n",
        "1. Definition: Gini Impurity measures the probability of misclassifying a randomly chosen instance from a dataset.\n",
        "2. Calculation: Gini Impurity is calculated as 1 - ∑(p^2), where p is the proportion of each class.\n",
        "3. Interpretation: Lower Gini Impurity values indicate a more pure node.\n",
        "\n",
        "Entropy\n",
        "1. Definition: Entropy measures the amount of uncertainty or randomness in a dataset.\n",
        "2. Calculation: Entropy is calculated as -∑(p * log2(p)), where p is the proportion of each class.\n",
        "3. Interpretation: Lower Entropy values indicate a more pure node.\n",
        "\n",
        "Impact on Decision Tree Splits\n",
        "1. Splitting criterion: Both Gini Impurity and Entropy are used as splitting criteria in Decision Trees.\n",
        "2. Node purity: The algorithm chooses the split that results in the largest reduction in impurity (Gini Impurity or Entropy).\n",
        "3. Tree structure: The choice of impurity measure can affect the structure of the Decision Tree.\n",
        "\n",
        "Comparison\n",
        "1. Similar results: Both Gini Impurity and Entropy often produce similar results.\n",
        "2. Computational efficiency: Gini Impurity is generally faster to compute than Entropy.\n",
        "\n",
        "Importance\n",
        "1. Decision Tree performance: The choice of impurity measure can impact the performance of the Decision Tree.\n",
        "2. Handling complex data: Understanding impurity measures is crucial for handling complex datasets.\n",
        "\n"
      ],
      "metadata": {
        "id": "w2uGoNlD-t2M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.\n",
        "\n",
        "Answer-3. Pre-Pruning\n",
        "1. Definition: Pre-pruning involves stopping the growth of a Decision Tree before it is fully grown, based on certain criteria.\n",
        "2. Criteria: Examples include maximum depth, minimum number of samples per node, or minimum impurity decrease.\n",
        "3. Practical advantage: Reduces computational cost and prevents overfitting by avoiding unnecessary splits.\n",
        "\n",
        "Post-Pruning\n",
        "1. Definition: Post-pruning involves growing a full Decision Tree and then removing branches that do not contribute significantly to the model's accuracy.\n",
        "2. Process: The tree is pruned back to a point where the error rate is minimized.\n",
        "3. Practical advantage: Allows for a more optimal pruning strategy, as the entire tree is considered before pruning.\n",
        "\n",
        "Key differences\n",
        "1. Timing: Pre-pruning occurs during tree growth, while post-pruning occurs after the tree is fully grown.\n",
        "2. Approach: Pre-pruning is a more conservative approach, while post-pruning is more flexible.\n"
      ],
      "metadata": {
        "id": "QzCusJuF_Lc_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gb90qHp9-mxX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n",
        "\n",
        "Answer-4. Information Gain measures the reduction in uncertainty or entropy in a dataset after splitting it based on a particular feature.\n",
        "\n",
        "Calculation\n",
        "Information Gain is calculated as:\n",
        "\n",
        "IG = Entropy(parent) - Entropy(child)\n",
        "\n",
        "Importance\n",
        "1. Choosing the best split: Information Gain helps determine the most informative feature to split the data.\n",
        "2. Reducing uncertainty: By maximizing Information Gain, the algorithm reduces uncertainty and improves the purity of the nodes.\n",
        "\n",
        "Role in Decision Trees\n",
        "1. Feature selection: Information Gain is used to select the most relevant features for splitting.\n",
        "2. Tree construction: By choosing the feature with the highest Information Gain, the algorithm constructs an optimal Decision Tree.\n",
        "\n",
        "Benefits\n",
        "1. Improved accuracy: Information Gain helps improve the accuracy of the Decision Tree model.\n",
        "2. Efficient splitting: By selecting the most informative features, Information Gain enables efficient splitting and reduces the risk of overfitting.\n"
      ],
      "metadata": {
        "id": "rSdxTpqK_h3k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?\n",
        "\n",
        "Answer-5. Real-World Applications\n",
        "1. Credit Risk Assessment: Decision Trees are used to evaluate creditworthiness and predict loan defaults.\n",
        "2. Medical Diagnosis: Decision Trees help diagnose diseases and identify high-risk patients.\n",
        "3. Customer Segmentation: Decision Trees are used to segment customers based on demographic and behavioral characteristics.\n",
        "4. Marketing and Sales: Decision Trees help predict customer responses to marketing campaigns and identify potential sales opportunities.\n",
        "\n",
        "Advantages\n",
        "1. Interpretability: Decision Trees are easy to understand and interpret.\n",
        "2. Handling categorical features: Decision Trees can handle categorical features directly.\n",
        "3. Fast training: Decision Trees are relatively fast to train.\n",
        "\n",
        "Limitations\n",
        "1. Overfitting: Decision Trees can overfit the training data, especially if they are too complex.\n",
        "2. Instability: Small changes in the data can result in significantly different Decision Trees.\n",
        "3. Limited handling of complex relationships: Decision Trees can struggle with complex relationships between features.\n",
        "\n",
        "Mitigating Limitations\n",
        "1. Pruning: Pruning techniques can help prevent overfitting.\n",
        "2. Ensemble methods: Combining multiple Decision Trees (e.g., Random Forests) can improve stability and accuracy.\n",
        "\n",
        "Dataset Information\n",
        "Iris Dataset\n",
        "- Classification task: Predict the species of an iris flower based on its features.\n",
        "- Features:\n",
        "    - Sepal length\n",
        "    - Sepal width\n",
        "    - Petal length\n",
        "    - Petal width\n",
        "- Target variable: Species (Setosa, Versicolor, or Virginica)\n",
        "- Number of samples: 150\n",
        "- Dataset source: sklearn.datasets.load_iris() or provided CSV\n",
        "\n",
        "Boston Housing Dataset\n",
        "- Regression task: Predict the median house price based on features of the neighborhood.\n",
        "- Features:\n",
        "    - CRIM (crime rate)\n",
        "    - ZN (proportion of residential land zoned for large lots)\n",
        "    - INDUS (proportion of non-retail business acres)\n",
        "    - CHAS (Charles River dummy variable)\n",
        "    - NOX (nitrogen oxides concentration)\n",
        "    - RM (average number of rooms per dwelling)\n",
        "    - AGE (proportion of owner-occupied units built prior to 1940)\n",
        "    - DIS (weighted distances to five Boston employment centers)\n",
        "    - RAD (index of accessibility to radial highways)\n",
        "    - TAX (full-value property tax rate)\n",
        "    - PTRATIO (pupil-teacher ratio)\n",
        "    - B (proportion of black population)\n",
        "    - LSTAT (percentage of lower status population)\n",
        "- Target variable: Median house price (MEDV)\n",
        "- Number of samples: 506\n",
        "- Dataset source: sklearn.datasets.load_boston() or provided CSV\n"
      ],
      "metadata": {
        "id": "jQX8wVI2ACf6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6:   Write a Python program to:\n",
        "\n",
        " ● Load the Iris Dataset\n",
        "\n",
        " ● Train a Decision Tree Classifier using the Gini criterion\n",
        "\n",
        " ● Print the model’s accuracy and feature importances\n",
        "\n",
        " (Include your Python code and output in the code box below.)\n",
        "\n",
        " Answer-6."
      ],
      "metadata": {
        "id": "auF9w9E0AXTj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Decision Tree Classifier using the Gini criterion\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate and print the model's accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "\n",
        "# Print feature importances\n",
        "feature_importances = clf.feature_importances_\n",
        "print(\"Feature Importances:\")\n",
        "for i, feature in enumerate(iris.feature_names):\n",
        "    print(f\"{feature}: {feature_importances[i]:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "foUSLZNVCpDM",
        "outputId": "26190503-fbcb-48e2-9c97-50dd17e098d6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n",
            "Feature Importances:\n",
            "sepal length (cm): 0.000\n",
            "sepal width (cm): 0.017\n",
            "petal length (cm): 0.906\n",
            "petal width (cm): 0.077\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7:  Write a Python program to:\n",
        "\n",
        "● Load the Iris Dataset\n",
        "\n",
        "● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to a fully-grown tree. (Include your Python code and output in the code box below.)\n",
        "\n",
        "Answert-7.  "
      ],
      "metadata": {
        "id": "P_2JmzQpBmXq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a fully-grown Decision Tree Classifier\n",
        "full_tree = DecisionTreeClassifier(random_state=42)\n",
        "full_tree.fit(X_train, y_train)\n",
        "y_pred_full = full_tree.predict(X_test)\n",
        "full_tree_accuracy = accuracy_score(y_test, y_pred_full)\n",
        "print(\"Fully-Grown Tree Accuracy:\", full_tree_accuracy)\n",
        "\n",
        "# Train a Decision Tree Classifier with max_depth=3\n",
        "limited_depth_tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "limited_depth_tree.fit(X_train, y_train)\n",
        "y_pred_limited = limited_depth_tree.predict(X_test)\n",
        "limited_depth_accuracy = accuracy_score(y_test, y_pred_limited)\n",
        "print(\"Limited Depth Tree (max_depth=3) Accuracy:\", limited_depth_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2QvLEU7CAqI",
        "outputId": "b09fa899-7c96-4fae-c4cc-f5d1cd353cc9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fully-Grown Tree Accuracy: 1.0\n",
            "Limited Depth Tree (max_depth=3) Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Write a Python program to:\n",
        "\n",
        "● Load the Boston Housing Dataset\n",
        "\n",
        "● Train a Decision Tree Regressor\n",
        "\n",
        " ● Print the Mean Squared Error (MSE) and feature importances (Include your Python code and output in the code box below.)\n",
        "\n",
        " Answer-8."
      ],
      "metadata": {
        "id": "-7QmFfZXC-pK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the Boston Housing Dataset\n",
        "boston = fetch_openml(name=\"boston\", version=1)\n",
        "X = boston.data\n",
        "y = boston.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# Calculate and print the Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n",
        "\n",
        "# Print feature importances\n",
        "feature_importances = regressor.feature_importances_\n",
        "print(\"Feature Importances:\")\n",
        "for i, feature in enumerate(boston.feature_names):\n",
        "    print(f\"{feature}: {feature_importances[i]:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZiNKGA6DFpn",
        "outputId": "dd697e83-b5c0-436c-c4f1-d9ba51ba487b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 10.416078431372549\n",
            "Feature Importances:\n",
            "CRIM: 0.051\n",
            "ZN: 0.003\n",
            "INDUS: 0.006\n",
            "CHAS: 0.000\n",
            "NOX: 0.027\n",
            "RM: 0.600\n",
            "AGE: 0.014\n",
            "DIS: 0.071\n",
            "RAD: 0.002\n",
            "TAX: 0.012\n",
            "PTRATIO: 0.011\n",
            "B: 0.009\n",
            "LSTAT: 0.193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Write a Python program to:\n",
        "\n",
        " ● Load the Iris Dataset\n",
        "\n",
        " ● Tune the Decision Tree’s max_depth and min_samples_split using GridSearchCV\n",
        "\n",
        " ● Print the best parameters and the resulting model accuracy (Include your Python code and output in the code box below.)\n",
        "\n",
        " Answer-9."
      ],
      "metadata": {
        "id": "kPIdJtC9DZPB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define hyperparameter tuning space\n",
        "param_grid = {\n",
        "    'max_depth': [1, 2, 3, 4, 5],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "# Perform GridSearchCV\n",
        "grid_search = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "# Train a Decision Tree Classifier with the best parameters\n",
        "best_model = DecisionTreeClassifier(**grid_search.best_params_, random_state=42)\n",
        "best_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate and print the model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8nFD-WrDtB4",
        "outputId": "fb710aae-f621-4996-8b04-ccbb881c234f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 2}\n",
            "Model Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Imagine you’re working as a data scientist for a healthcare company that wants to predict whether a patient has a certain disease. You have a large dataset with mixed data types and some missing values.\n",
        "\n",
        " Explain the step-by-step process you would follow to:\n",
        "\n",
        " ● Handle the missing values ● Encode the categorical features\n",
        "\n",
        "  ● Train a Decision Tree model ● Tune its hyperparameters\n",
        "  \n",
        "  ● Evaluate its performance And describe what business value this model could provide in the real-world setting.\n",
        "\n",
        "  Answer-10. Step-by-Step Process\n",
        "Handling Missing Values\n",
        "1. Identify missing values: Determine the extent of missing values in the dataset.\n",
        "2. Imputation methods: Choose suitable imputation methods, such as mean, median, or imputation using regression.\n",
        "3. Impute missing values: Apply the chosen imputation method to replace missing values.\n",
        "\n",
        "Encoding Categorical Features\n",
        "1. Identify categorical features: Determine which features are categorical.\n",
        "2. Encoding methods: Choose suitable encoding methods, such as one-hot encoding, label encoding, or ordinal encoding.\n",
        "3. Encode categorical features: Apply the chosen encoding method to transform categorical features into numerical features.\n",
        "\n",
        "Training a Decision Tree Model\n",
        "1. Split data: Split the dataset into training and testing sets.\n",
        "2. Train Decision Tree: Train a Decision Tree model on the training data.\n",
        "\n",
        "Tuning Hyperparameters\n",
        "1. Define hyperparameter space: Define the hyperparameter space for tuning, including parameters like max_depth, min_samples_split, and min_samples_leaf.\n",
        "2. Hyperparameter tuning method: Choose a hyperparameter tuning method, such as GridSearchCV or RandomSearchCV.\n",
        "3. Tune hyperparameters: Perform hyperparameter tuning using the chosen method.\n",
        "\n",
        "Evaluating Model Performance\n",
        "1. Metrics: Choose suitable evaluation metrics, such as accuracy, precision, recall, F1-score, and AUC-ROC.\n",
        "2. Evaluate model: Evaluate the model's performance on the testing data using the chosen metrics.\n",
        "\n",
        "Business Value\n",
        "1. Disease prediction: The model can predict the likelihood of a patient having a certain disease, enabling early intervention and treatment.\n",
        "2. Improved patient outcomes: Accurate predictions can lead to better patient outcomes, reduced morbidity, and mortality.\n",
        "3. Resource optimization: The model can help optimize resource allocation, reducing unnecessary tests and procedures.\n",
        "4. Enhanced patient care: The model can provide valuable insights for healthcare professionals, enabling more informed decision-making and personalized patient care.\n",
        "\n",
        "By following this step-by-step process, the healthcare company can develop a robust Decision Tree model that provides valuable predictions and insights, ultimately improving patient outcomes and resource allocation."
      ],
      "metadata": {
        "id": "fOWiXGB1D_KL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "//////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////"
      ],
      "metadata": {
        "id": "MoVC-PWpE-Uc"
      }
    }
  ]
}